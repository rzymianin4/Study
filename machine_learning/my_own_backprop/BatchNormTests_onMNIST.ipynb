{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COL_NAMES': ['label', 'data'],\n",
       " 'DESCR': 'mldata.org dataset: mnist-original',\n",
       " 'data': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " 'target': array([0., 0., 0., ..., 9., 9., 9.])}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import Layers as ly\n",
    "# from Layers import Layer, Trainable\n",
    "from tqdm import *\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "\n",
    "\n",
    "class GlobalVariables(object):\n",
    "    def __init__(self):\n",
    "        self.trainables = []\n",
    "\n",
    "_GLOBALS = GlobalVariables()\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        raise Exception(\"This is pure virtual method\")\n",
    "\n",
    "    def backward(self, delta):\n",
    "        raise Exception(\"This is pure virtual method\")\n",
    "\n",
    "\n",
    "\n",
    "class Trainable(object):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._variables = {}\n",
    "        self._gradients = {}\n",
    "        _GLOBALS.trainables.append(self)\n",
    "\n",
    "    def train(self):\n",
    "        for i in self._variables:\n",
    "            # print(i, self._variables[i].shape, self._gradients[i].shape)\n",
    "            self._variables[i] -= self._gradients[i]\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "\n",
    "\n",
    "class FullyConnected(Layer, Trainable):\n",
    "    def __init__(self, prev, size, init_fun=np.random.uniform, init_args={\"low\":-0.05, \"high\":0.05}):\n",
    "        super().__init__()\n",
    "        self.x = None\n",
    "        self._prev = prev\n",
    "        self._variables[\"weights\"] = init_fun(**init_args, size=[prev.shape[-1], size])\n",
    "        self.shape = tuple([prev.shape[0], size])\n",
    "        self._gradients[\"weights\"] = np.zeros(shape=[prev.shape[-1], size])\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        self.x = self._prev.forward(**kwargs)\n",
    "        self.x = self.x @ self._variables[\"weights\"]\n",
    "        return self.x\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        self_delta = delta @ self._variables[\"weights\"].T\n",
    "        self._gradients[\"weights\"] = self._prev.x.T @ delta\n",
    "        self._prev.backward(self_delta)\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "\n",
    "\n",
    "class Placeholder(Layer):\n",
    "    def __init__(self, sizes):\n",
    "        self.x = None\n",
    "        self.shape = tuple(sizes)\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        return self.x\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        pass\n",
    "    \n",
    "    def feed(self, data):\n",
    "        if data.shape[1:] != self.shape[1:]:\n",
    "            raise Exception(\"Bad sizes\")\n",
    "        self.x = data\n",
    "\n",
    "\n",
    "\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, prev, keep_prob=1.0):\n",
    "        self.x = None\n",
    "        self._prev = prev\n",
    "        self._keep_prob = keep_prob\n",
    "        self.shape = tuple(prev.shape)\n",
    "        self._filter = None\n",
    "        \n",
    "    def forward(self, train=False, **kwargs):\n",
    "        self.x = self._prev.forward(train=train, **kwargs)\n",
    "        if train:\n",
    "            self._filter = np.random.binomial(1, 1-self._keep_prob, size=self.x.shape)\n",
    "            self.x[self._filter] = 0\n",
    "            self.x /= self._keep_prob\n",
    "        return self.x\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        self_delta = delta * self._keep_prob\n",
    "        self_delta[self._filter] = 0\n",
    "        self._prev.backward(self_delta)\n",
    "\n",
    "\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def __init__(self, prev):\n",
    "        self.x = None\n",
    "        self._prev = prev\n",
    "        self.shape = tuple(prev.shape)\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        self.x = self._prev.forward(**kwargs)\n",
    "        self.x *= (self.x > 0).astype(np.int8)\n",
    "        return self.x\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        self_delta = delta*(self._prev.x > 0).astype(np.int8)\n",
    "        self._prev.backward(self_delta)\n",
    "\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self, prev):\n",
    "        self.x = None\n",
    "        self._prev = prev\n",
    "        self.shape = tuple(prev.shape)\n",
    "    \n",
    "    def forward(self, **kwargs):\n",
    "        self.x = self._prev.forward(**kwargs)\n",
    "        self.x = 1/(1 + np.exp(-self.x))\n",
    "        return self.x\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        self_delta = self.x*(1-self.x)*delta\n",
    "        self._prev.backward(self_delta)\n",
    "\n",
    "\n",
    "\n",
    "class Softmax(Layer):\n",
    "    def __init__(self, prev):\n",
    "        self.x = None\n",
    "        self._prev = prev\n",
    "        self.shape = tuple(prev.shape)\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        self.x = self._prev.forward(**kwargs)\n",
    "        self.x = np.exp(self.x)\n",
    "        self.x /= np.sum(self.x, -1)[:,None]\n",
    "        return self.x\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        self_delta = np.zeros(delta.shape)\n",
    "        for i in range(self_delta.shape[1]):\n",
    "            for k in range(self_delta.shape[1]):\n",
    "                self_delta[:, i] += delta[:,k]*(self.x[:,i]*(1-self.x[:,i]) if i==k else -self.x[:,i]*self.x[:,k])\n",
    "        self._prev.backward(self_delta)\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "\n",
    "class SoftmaxCrossEntropy(object):\n",
    "    def __init__(self, predicts, labels, learn_rate=0.001):\n",
    "        if predicts.shape[1:] != labels.shape[1:]:\n",
    "            raise Exception(\"Bad sizes\")\n",
    "        self._predicts = predicts\n",
    "        self._labels = labels\n",
    "        self._learn_rate = learn_rate\n",
    "        \n",
    "    @staticmethod\n",
    "    def _softmax(x):\n",
    "        s = np.exp(x)\n",
    "        s /= np.sum(s, -1)[:,None]\n",
    "        return s*(1-1e-8) # to not obtain 0 or 1 for numerical stable\n",
    "    \n",
    "    @staticmethod\n",
    "    def _cross_entropy(s, y):\n",
    "        loss = -np.sum(y*np.log(s) + (1-y)*np.log(1-s))/y.shape[0]\n",
    "        return np.nan_to_num(loss)\n",
    "    \n",
    "    def run(self):\n",
    "        y = self._labels.forward()\n",
    "        x = self._predicts.forward(train=True)\n",
    "        s = SoftmaxCrossEntropy._softmax(x)\n",
    "        delta = -y + (1-y)*s/(1-s)\n",
    "        for k in range(delta.shape[1]):\n",
    "            if s[:,k].max() == 1:\n",
    "                print(s[:,k])\n",
    "            delta[:,:] += (y-s)[:,k][:,None] * (s/(1-s[:,k][:,None]))\n",
    "        self._predicts.backward((1/y.shape[0])*np.nan_to_num(delta)*self._learn_rate)\n",
    "\n",
    "        for trainable in _GLOBALS.trainables:\n",
    "            trainable.train()\n",
    "\n",
    "        return SoftmaxCrossEntropy._cross_entropy(s, y)\n",
    "\n",
    "\n",
    "\n",
    "class SigmoidCrossEntropy(object):\n",
    "    def __init__(self, predicts, labels, learn_rate=0.001):\n",
    "        if predicts.shape[1:] != labels.shape[1:]:\n",
    "            raise Exception(\"Bad sizes\")\n",
    "        self._predicts = predicts\n",
    "        self._labels = labels\n",
    "        self._learn_rate = learn_rate\n",
    "        \n",
    "    @staticmethod\n",
    "    def _sigmoid(x):\n",
    "        sigma = 1/(1 + np.exp(-x))\n",
    "        return sigma*(1-1e-8) # to not obtain 0 or 1 for numerical stable\n",
    "    \n",
    "    @staticmethod\n",
    "    def _cross_entropy(sigma, y):\n",
    "        loss = -np.sum(y*np.log(sigma) + (1-y)*np.log(1-sigma))/y.shape[0]\n",
    "        return np.nan_to_num(loss)\n",
    "    \n",
    "    def run(self):\n",
    "        y = self._labels.forward()\n",
    "        x = self._predicts.forward(train=True)\n",
    "        sigma = SigmoidCrossEntropy._softmax(x)\n",
    "        delta = sigma - y\n",
    "        self._predicts.backward((1/y.shape[0])*np.nan_to_num(delta)*self._learn_rate)\n",
    "\n",
    "        for trainable in _GLOBALS.trainables:\n",
    "            trainable.train()\n",
    "\n",
    "        return SigmoidCrossEntropy._cross_entropy(sigma, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization(Layer, Trainable):\n",
    "    def _checkParams(self, dims, prevshape):\n",
    "        if type(dims) is int:\n",
    "            dims = tuple([dims])\n",
    "        else:\n",
    "            dims = tuple(np.sort(dims))\n",
    "        if len(prevshape) < dims[-1]+1:\n",
    "            raise Exception(\"x has to few dimensions\")\n",
    "        self._dims = dims\n",
    "        prevshape = list(prevshape[:])\n",
    "        for d in dims:\n",
    "            prevshape[d] = 1\n",
    "        self._statshape = tuple(prevshape[:])\n",
    "        prevshape[0] = 1\n",
    "        return prevshape\n",
    "    \n",
    "    \n",
    "    def __init__(self, prev, dims, epsi=1e-8, ema=0.99, init_fun=np.random.uniform, init_args={\"low\":-0.05, \"high\":0.05}):\n",
    "        super().__init__()\n",
    "        varshape = self._checkParams(dims, prev.shape)\n",
    "        self.x = None\n",
    "        self._prev = prev\n",
    "        self.shape = prev.shape[:]\n",
    "        self._variables[\"gamma\"] = init_fun(**init_args, size=varshape) + 1\n",
    "        self._variables[\"beta\"] = init_fun(**init_args, size=varshape)\n",
    "        self._gradients[\"gamma\"] = np.zeros(shape=varshape) + 1\n",
    "        self._gradients[\"beta\"] = np.zeros(shape=varshape)\n",
    "        self._validvars = {}\n",
    "        self._validvars[\"mu\"] = np.zeros(shape=varshape) + 1\n",
    "        self._validvars[\"sigma\"] = np.zeros(shape=varshape)\n",
    "        self._epsi = epsi\n",
    "        self._ema = ema\n",
    "        self._d_dx = None\n",
    "        self._d_dg = None\n",
    "        # print(self._dims, self._variables[\"gamma\"].shape, self._gradients[\"gamma\"].shape)\n",
    "        \n",
    "    def forward(self, train=False, **kwargs):\n",
    "        self.x = self._prev.forward(train=train, **kwargs)\n",
    "        # print(\"train\", train)\n",
    "        if train:\n",
    "            mu = self.x.mean(self._dims).reshape(self._statshape)\n",
    "            sigma = self.x.var(self._dims).reshape(self._statshape)\n",
    "            # print(\"mu, simg\", mu.shape, sigma.shape)\n",
    "            cntr = (self.x - mu)\n",
    "            denom = np.sqrt(sigma + self._epsi)\n",
    "            # print(\"cntr, denom\", cntr.shape, denom.shape)\n",
    "            self._d_dx = self._variables[\"gamma\"]/denom\n",
    "            self._d_dg = np.expand_dims(cntr.mean(0), 0)/denom\n",
    "            # print(\"self._d_dx, self._d_dg\", self._d_dx.shape, self._d_dg.shape)\n",
    "            self.x = self._d_dx*cntr + self._variables[\"beta\"]\n",
    "            if 0 not in self._dims:\n",
    "                mu = np.expand_dims(mu.mean(0), 0)\n",
    "                sigma = np.expand_dims(sigma.mean(0), 0)\n",
    "                self._d_dx = np.expand_dims(self._d_dx.mean(0), 0)\n",
    "                self._d_dg = np.expand_dims(self._d_dg.mean(0), 0)\n",
    "            self._validvars[\"mu\"] = self._ema*self._validvars[\"mu\"] + (1-self._ema)*mu\n",
    "            self._validvars[\"sigma\"] = self._ema*self._validvars[\"sigma\"] + (1-self._ema)*sigma\n",
    "        else:\n",
    "            # print(\"test\")\n",
    "            a = self._variables[\"gamma\"]/np.sqrt(self._validvars[\"sigma\"] + self._epsi)\n",
    "            self.x = a*(self.x - self._validvars[\"mu\"]) + self._variables[\"beta\"]\n",
    "        return self.x\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        self_delta = delta * self._d_dx\n",
    "        delta = np.mean(delta, self._dims).reshape(self._gradients[\"beta\"].shape)\n",
    "        self._gradients[\"beta\"] = delta\n",
    "        self._gradients[\"gamma\"] = delta * self._d_dg\n",
    "        # print(self._dims, self._variables[\"gamma\"].shape, self._gradients[\"gamma\"].shape, self._d_dg.shape)\n",
    "        # print(self._dims, self._variables[\"beta\"].shape, self._gradients[\"beta\"].shape, self._d_dg.shape)\n",
    "        self._prev.backward(self_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "y_ = y.astype(int)\n",
    "y = np.zeros([y_.shape[0], 10])\n",
    "y[np.arange(y_.shape[0]), y_] = 1\n",
    "X = X.astype(np.float32)/255\n",
    "shuffle_index = np.random.permutation(X.shape[0])\n",
    "X, y = X[shuffle_index], y[shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(imgs, lbls, last, X, y, bs=-1):\n",
    "    if bs == -1:\n",
    "        bs = y.shape[0]\n",
    "    score = 0\n",
    "    for i in range(int(X.shape[0]/bs)):\n",
    "        imgs.feed(X[i*bs:(i+1)*bs])\n",
    "        preds = last.forward();\n",
    "        res = np.argmax(preds, -1)\n",
    "        lbls = np.argmax(y[i*bs:(i+1)*bs], -1)\n",
    "        score += np.sum(res == lbls)\n",
    "    return score/y.shape[0]\n",
    "        \n",
    "#     imgs.feed(X)\n",
    "#     preds = last.forward();\n",
    "#     res = np.argmax(preds, -1)\n",
    "#     lbls = np.argmax(y, -1)\n",
    "#     return np.sum(res == lbls)/lbls.shape[0]\n",
    "    \n",
    "def train(imgs, lbls, cost, n_ep, X, y, bs):\n",
    "    for e in range(n_ep):\n",
    "        shuffle_index = np.random.permutation(X.shape[0])\n",
    "        X, y = X[shuffle_index], y[shuffle_index]\n",
    "        loss = []\n",
    "        for i in range(int(X.shape[0]/bs)):\n",
    "        #for i in range(int(X.shape[0]/bs)):\n",
    "            imgs.feed(X[i*bs:(i+1)*bs])\n",
    "            lbls.feed(y[i*bs:(i+1)*bs])\n",
    "            loss.append(cost.run())\n",
    "        print(\"\\repoch {0} done; mean loss = {1:.5f}\".format(e+1, np.array(loss).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAPES = [800, 200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BatchNormalization before 2nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = Placeholder([-1, 784])\n",
    "lbls = Placeholder([-1, 10])\n",
    "\n",
    "x = FullyConnected(imgs, SHAPES[0])\n",
    "x = Dropout(x, 0.9)\n",
    "x = ReLU(x)\n",
    "x = BatchNormalization(x, [0])\n",
    "x = FullyConnected(x, SHAPES[1])\n",
    "x = Dropout(x, 0.9)\n",
    "x = ReLU(x)\n",
    "# x = BatchNormalization(x, [0])\n",
    "x = FullyConnected(x, 10)\n",
    "\n",
    "s = Softmax(x)\n",
    "crsEnt = SoftmaxCrossEntropy(x, lbls, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 done; mean loss = 0.55612\n",
      "epoch 2 done; mean loss = 0.25858\n",
      "epoch 3 done; mean loss = 0.19741\n",
      "epoch 4 done; mean loss = 0.16194\n",
      "epoch 5 done; mean loss = 0.13894\n",
      "epoch 6 done; mean loss = 0.11881\n",
      "epoch 7 done; mean loss = 0.10367\n",
      "epoch 8 done; mean loss = 0.09440\n",
      "epoch 9 done; mean loss = 0.08591\n",
      "epoch 10 done; mean loss = 0.07962\n"
     ]
    }
   ],
   "source": [
    "train(imgs, lbls, crsEnt, 10, X[:55000], y[:55000], 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 1\n",
      "\ttrain score: 0.9966\n",
      "\tvalid score: 0.9777\n",
      "\ttest score:  0.9794\n",
      "batch size: 4\n",
      "\ttrain score: 0.9966\n",
      "\tvalid score: 0.9777\n",
      "\ttest score:  0.9794\n",
      "batch size: 128\n",
      "\ttrain score: 0.9950181818181818\n",
      "\tvalid score: 0.9761\n",
      "\ttest score:  0.9778\n",
      "batch size: inf\n",
      "\ttrain score: 0.9966\n",
      "\tvalid score: 0.9777\n",
      "\ttest score:  0.9794\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [1,4,128,-1]\n",
    "for bs in batch_sizes:\n",
    "    print(\"batch size:\", bs if bs > 0 else \"inf\")\n",
    "    print(\"\\ttrain score:\", test(imgs, lbls, s, X[:55000], y[:55000], bs))\n",
    "    print(\"\\tvalid score:\", test(imgs, lbls, s, X[55000:65000], y[55000:65000], bs))\n",
    "    print(\"\\ttest score: \", test(imgs, lbls, s, X[65000:], y[65000:], bs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BatchNormalization before 3rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = Placeholder([-1, 784])\n",
    "lbls = Placeholder([-1, 10])\n",
    "\n",
    "x = FullyConnected(imgs, SHAPES[0])\n",
    "x = Dropout(x, 0.9)\n",
    "x = ReLU(x)\n",
    "# x = BatchNormalization(x, [0])\n",
    "x = FullyConnected(x, SHAPES[1])\n",
    "x = Dropout(x, 0.9)\n",
    "x = ReLU(x)\n",
    "x = BatchNormalization(x, [0])\n",
    "x = FullyConnected(x, 10)\n",
    "\n",
    "s = Softmax(x)\n",
    "crsEnt = SoftmaxCrossEntropy(x, lbls, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 done; mean loss = 0.59711\n",
      "epoch 2 done; mean loss = 0.33537\n",
      "epoch 3 done; mean loss = 0.26523\n",
      "epoch 4 done; mean loss = 0.22394\n",
      "epoch 5 done; mean loss = 0.19512\n",
      "epoch 6 done; mean loss = 0.17535\n",
      "epoch 7 done; mean loss = 0.15707\n",
      "epoch 8 done; mean loss = 0.14490\n",
      "epoch 9 done; mean loss = 0.13193\n",
      "epoch 10 done; mean loss = 0.12268\n"
     ]
    }
   ],
   "source": [
    "train(imgs, lbls, crsEnt, 10, X[:55000], y[:55000], 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 1\n",
      "\ttrain score: 0.9917636363636364\n",
      "\tvalid score: 0.9765\n",
      "\ttest score:  0.98\n",
      "batch size: 4\n",
      "\ttrain score: 0.9917636363636364\n",
      "\tvalid score: 0.9765\n",
      "\ttest score:  0.98\n",
      "batch size: 128\n",
      "\ttrain score: 0.9901818181818182\n",
      "\tvalid score: 0.975\n",
      "\ttest score:  0.9786\n",
      "batch size: inf\n",
      "\ttrain score: 0.9917636363636364\n",
      "\tvalid score: 0.9765\n",
      "\ttest score:  0.98\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [1,4,128,-1]\n",
    "for bs in batch_sizes:\n",
    "    print(\"batch size:\", bs if bs > 0 else \"inf\")\n",
    "    print(\"\\ttrain score:\", test(imgs, lbls, s, X[:55000], y[:55000], bs))\n",
    "    print(\"\\tvalid score:\", test(imgs, lbls, s, X[55000:65000], y[55000:65000], bs))\n",
    "    print(\"\\ttest score: \", test(imgs, lbls, s, X[65000:], y[65000:], bs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BatchNormalization before all hiden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = Placeholder([-1, 784])\n",
    "lbls = Placeholder([-1, 10])\n",
    "\n",
    "x = FullyConnected(imgs, SHAPES[0])\n",
    "x = Dropout(x, 0.9)\n",
    "x = ReLU(x)\n",
    "x = BatchNormalization(x, [0])\n",
    "x = FullyConnected(x, SHAPES[1])\n",
    "x = Dropout(x, 0.9)\n",
    "x = ReLU(x)\n",
    "x = BatchNormalization(x, [0])\n",
    "x = FullyConnected(x, 10)\n",
    "\n",
    "s = Softmax(x)\n",
    "crsEnt = SoftmaxCrossEntropy(x, lbls, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 done; mean loss = 0.49949\n",
      "epoch 2 done; mean loss = 0.25034\n",
      "epoch 3 done; mean loss = 0.19095\n",
      "epoch 4 done; mean loss = 0.15934\n",
      "epoch 5 done; mean loss = 0.13713\n",
      "epoch 6 done; mean loss = 0.12167\n",
      "epoch 7 done; mean loss = 0.10949\n",
      "epoch 8 done; mean loss = 0.10010\n",
      "epoch 9 done; mean loss = 0.09117\n",
      "epoch 10 done; mean loss = 0.08676\n"
     ]
    }
   ],
   "source": [
    "train(imgs, lbls, crsEnt, 10, X[:55000], y[:55000], 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 1\n",
      "\ttrain score: 0.9962181818181818\n",
      "\tvalid score: 0.9791\n",
      "\ttest score:  0.981\n",
      "batch size: 4\n",
      "\ttrain score: 0.9962181818181818\n",
      "\tvalid score: 0.9791\n",
      "\ttest score:  0.981\n",
      "batch size: 128\n",
      "\ttrain score: 0.9946181818181818\n",
      "\tvalid score: 0.9775\n",
      "\ttest score:  0.9798\n",
      "batch size: inf\n",
      "\ttrain score: 0.9962181818181818\n",
      "\tvalid score: 0.9791\n",
      "\ttest score:  0.981\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [1,4,128,-1]\n",
    "for bs in batch_sizes:\n",
    "    print(\"batch size:\", bs if bs > 0 else \"inf\")\n",
    "    print(\"\\ttrain score:\", test(imgs, lbls, s, X[:55000], y[:55000], bs))\n",
    "    print(\"\\tvalid score:\", test(imgs, lbls, s, X[55000:65000], y[55000:65000], bs))\n",
    "    print(\"\\ttest score: \", test(imgs, lbls, s, X[65000:], y[65000:], bs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = Placeholder([-1, 784])\n",
    "lbls = Placeholder([-1, 10])\n",
    "\n",
    "x = FullyConnected(imgs, SHAPES[0])\n",
    "x = Dropout(x, 0.9)\n",
    "x = ReLU(x)\n",
    "# x = BatchNormalization(x, [0])\n",
    "x = FullyConnected(x, SHAPES[1])\n",
    "x = Dropout(x, 0.9)\n",
    "x = ReLU(x)\n",
    "# x = BatchNormalization(x, [0])\n",
    "x = FullyConnected(x, 10)\n",
    "\n",
    "s = Softmax(x)\n",
    "crsEnt = SoftmaxCrossEntropy(x, lbls, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 done; mean loss = 1.29547\n",
      "epoch 2 done; mean loss = 0.52528\n",
      "epoch 3 done; mean loss = 0.41337\n",
      "epoch 4 done; mean loss = 0.34510\n",
      "epoch 5 done; mean loss = 0.29641\n",
      "epoch 6 done; mean loss = 0.25935\n",
      "epoch 7 done; mean loss = 0.23135\n",
      "epoch 8 done; mean loss = 0.20839\n",
      "epoch 9 done; mean loss = 0.18902\n",
      "epoch 10 done; mean loss = 0.17362\n"
     ]
    }
   ],
   "source": [
    "train(imgs, lbls, crsEnt, 10, X[:55000], y[:55000], 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 1\n",
      "\ttrain score: 0.9835454545454545\n",
      "\tvalid score: 0.972\n",
      "\ttest score:  0.973\n",
      "batch size: 4\n",
      "\ttrain score: 0.9835454545454545\n",
      "\tvalid score: 0.972\n",
      "\ttest score:  0.973\n",
      "batch size: 128\n",
      "\ttrain score: 0.9819636363636364\n",
      "\tvalid score: 0.9705\n",
      "\ttest score:  0.9716\n",
      "batch size: inf\n",
      "\ttrain score: 0.9835454545454545\n",
      "\tvalid score: 0.972\n",
      "\ttest score:  0.973\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [1,4,128,-1]\n",
    "for bs in batch_sizes:\n",
    "    print(\"batch size:\", bs if bs > 0 else \"inf\")\n",
    "    print(\"\\ttrain score:\", test(imgs, lbls, s, X[:55000], y[:55000], bs))\n",
    "    print(\"\\tvalid score:\", test(imgs, lbls, s, X[55000:65000], y[55000:65000], bs))\n",
    "    print(\"\\ttest score: \", test(imgs, lbls, s, X[65000:], y[65000:], bs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, dims, epsi=1e-8):\n",
    "    if type(d) is int:\n",
    "        d = tuple([d])\n",
    "    else:\n",
    "        dims = tuple(np.sort(dims))\n",
    "    if x.ndim < np.max(dims)+1:\n",
    "        raise Exception(\"x has to few dimensions\")\n",
    "    mu = x.mean(dims)\n",
    "    sigm = x.var(dims)\n",
    "    for d in dims:\n",
    "        mu = np.expand_dims(mu, d)\n",
    "        sigm = np.expand_dims(sigm, d)\n",
    "    return (x - mu)/np.sqrt(sigm + epsi)"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
